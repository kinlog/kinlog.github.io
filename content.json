{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"about","text":"","link":"/about/index.html"}],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/11/30/hello-world/"},{"title":"神经网络量化","text":"量化是指将连续分布的数据或者较大的数据集合转化为离散形式的处理。在信号处理、图片处理、数据压缩等方面，量化都有着诸多的应用，而本文将聚焦于神经网络中的量化。 背景现有的大多深度学习应用都是基于32位的浮点进行训练或者推断的。现有一些方法尝试使用低精度的表示完成运算，比如训练任务采用16bit乘法器，或者是在推断任务中使用8bit乘法器。数据从高精度表示到低精度表示的转换我们统称为量化（Quantization）。随着深度学习方法在业界的逐步落地，量化也逐渐成为算法落地过程中必经的步骤之一。 低精度表示带来的好处主要有两方面，其一是更低的内存带宽占用，实现快速的运算需要有高速的内存，而内存的存取速度与其价格成正比，所以内存大小能直接影响到成本；其二是更大的计算吞吐量，该指标一般使用每秒运算数（Operation per second，简称OPS）衡量，低精度运算所需要的芯片面积可以更小，例如Intel等芯片厂商已经开发了将4个INT8数据拼接为32bit进行运算的相关指令，这样同一芯片执行力INT8运算的OPS等价提升为32bit运算的四倍，非常可观。 量化一般分为两种，分别是训练后量化（Post-Training Quantization）和训练量化。前者是基于充分训练的浮点模型，将其权重或者激活值转化为低比特数表示。在转化之后，部分模型需要finetune实现更低的精度损失。而训练量化是基于一个未经充分训练（甚至是随机初始化）的模型，将其权重、激活值和梯度的部分或者全部转化为低比特表示，利用低比特表示进行训练，使得该模型的效果逼近浮点模型的效果。上述的分类方法只是从是否需要训练的角度进行划分，当然也可以从其他的角度进行分类，比如根据低比特表示的分布是否均匀可分为均匀量化和非均匀量化；根据低比特表示覆盖范围是否涵盖原始数据的范围可以分为饱和量化和非饱和量化。更细节的内容将在下面的章节介绍。 历史现有的量化算法基本可以实现基于16bit乘法器的无损训练，或者是基于8bit乘法器的无损推断。 基本量化方案现有的表示方案主要有以下三类：均匀量化（Uniform Quantization）、2的指数量化（Power of 2 Quantization）和浮点量化（Float Quantization）。 均匀量化均匀量化指的是通过一个 scale 参数，将原始数据 x 转化为该参数和低比特表示 x&#39; （一般为整形值）的乘积，即 scale*x&#39; 。 均匀量化一般有以下三个步骤： 搜索数据的极值 根据极值计算量化参数 量化转换 对于均匀量化，我们采用如下形式化表示。假设有一组分布在区间$[x_{min}, x_{max}]$的浮点数值需要量化到无符号的$(0,2^{b}-1)$范围内，其中$b$表示量化表示的比特数，比如对于 8bit 量化共有$2^8$个数值。将浮点值转化为整形量化表示的过程需要有两个参数：scale 和 zero-point（$z$） ，scale 表示量化的步长，是一种范围的伸缩变换；而 zero-point 则是一种范围的位移变换。zero-point 是一个整形值，用于确保0值的量化是没有误差的。 基于上述的问题，scale 和 z 的计算方式分别如下$$scale = \\frac{x_{max} - x_{min}}{2^{b}-1}$$$$z = -round(\\frac{x_{min}}{scale})$$一旦参数 scale 和 zero-point（$z$） 得以确定，量化就可以通过下面的方式进行：$$x’ = round(\\frac{x}{scale})+z$$$$x_{int} = clamp(x’, 0, 2^{b}-1)$$其中，$clamp(x, a, b)=min(max(a, x), b)$。而反量化就是上述的逆过程：$$x_{rec} = (x_{int} - z) \\times scale$$ 对称与非对称量化关于对称量化和非对称量化的区别，一种说法是通过量化的区间是否关于0对称来判断，还有一种说法是当$z=0$即是对称量化，反之则不是。姑且不论二者的正统与否，毕竟其内核是一致的。对称量化的操作一般如下： $$x’ = round(\\frac{x}{scale})$$$$x_{int} = clamp(x’, -2^{b-1}+1, 2^{b-1}-1), 当x是有符号数$$$$x_{int} = clamp(x’, 0, 2^b-1), 当x是无符号数$$ 对ReLU层输出的激活值可以做无符号的量化，而其他层激活值则一般只能是有符号的量化。 浮点步长与定点量化当 scale 参数是一个浮点数，此时的均匀量化可以称之为浮点步长均匀量化。当 scale 参数是 $2^n$ 这种表示时，该量化形式称之为定点量化。通过上述的方法所计算出的 scale 一般是一个浮点数，即对应了浮点步长量化。所以接下来介绍一种带符号位的动态定点的量化方法。 在介绍定点量化的流程之前，我们先来看下定点的表示方案。定点是将数据使用整形值 $x_{int}$ 和一个共享指数 $e$ 联合表示，即$$x_{rec}=x_{int} \\times 2^{e}$$定点量化的关键就是要找到合适的共享指数 $e$，使得总体的量化损失最小。一般地，我们认为大值是更为关键的，对神经网络的输出有着直接的影响，所以要保证大值尽可能地被覆盖。而小值的量化结果可能就是$x_{int}=0$，也就是以0替代。 根据上述基本策略，定点量化步骤如下 Step1，从原始数据搜索最大值：$$f=max(|x|)$$Step2，计算最大值的指数部分：$$e_{f}=floor(log2(f)) + 1$$Step3，根据最大值指数计算数据的共享指数：$$e=e_{f} - (b-1)$$Step4，数据变换：$$x_{int}=round(x/2^e)$$其中，$b$表示整形值部分的比特数，而$b-1$则是去掉符号位一个比特之后的比特数。一般地，还需要对量化的结果约束在$b$个比特可以表示的范围内，即$clamp(x_{int}, -2^{b-1}-1, 2^{b-1}-1)$。 浮点量化浮点量化就是采用低精度的浮点格式替换高精度的浮点数。在解释浮点量化之前，首先介绍下浮点数的格式。浮点是相对于定点而言的，即其小数点的位置是不固定的。根据IEEE 754标准，浮点数的表示可以分为三个部分，分别是：符号位（sign）、尾数（mantissa）和指数（exponent）。这三个部分的解释如下： 符号位（记做$S$）由一个比特表示，实际表示的值为$(-1)^S$，当$S=0$表示该浮点数为正数；当$S=1$表示该浮点数为负数 尾数部分（记做$M$）有若干比特，首位含有一个隐藏的比特，实际表示的值是介于1到2之间的小数 指数部分（记做$E$）有若干比特，实际表示的值为$2^E$ 浮点数的所存储的值就是三者的乘积，即$$Float_value = (-1)^S \\times M \\times 2^{E}$$ IEEE 754标准还规定的了现今两种使用最多的浮点数格式，分别是32比特的单精度浮点（Single precision）和64比特的双精度浮点（Double precision）。单精度浮点的32比特划分为（1，8，23）三个部分，分别是符号位、指数部分和尾数部分的比特数。 尾数部分按照如下的方式解析。该部分的第一个比特表示$2^{-1}$，第二个比特表示$2^{-2}$，以此类推。另外尾数部分隐含了$1.0$，所以尾数部分的真实值是 $1.0 + m_{1} \\times 2^{-1} + m_{2} \\times 2^{-2} + …$，其中$m_{i}$表示尾数部分第 $i$ 个比特的值。 指数部分的所有比特构成一个无符号的整形数（记做$U$），比如单精度浮点的指数部分有8个比特，那么该无符号整形数的取值范围为 $U\\in[0,255]$。为了同时可以兼顾大值和小值的表示，该无符号整形和真实的$E$之间存在一个偏置。对于单精度浮点，该偏置的值为127。直白地说，$E=U-127$，因此有$E\\in[-127,128]$。尽管理论上$E$是该取值范围，但按照刚才的方法，$(-1)^S \\times M \\times 2^{E}$是没有办法准确表示 0 的。实际上$E$的解析还存在两种特殊情况： 指数$E$所有比特全为0。此时，浮点数指数的$E$等于 1-127，尾数部分$M$不再包含隐含的$1.0$。这样就可以表示 $\\pm 0$和一些很小的数。 指数$E$所有比特全为1。此时，如果尾数$M$所有比特全为0，表示无穷大$\\pm \\inf$；如果尾数$M$不全为0，表示该数不是一个数（即NaN）。 双精度浮点共64比特，含1个比特的符号位、11个比特的指数和52个比特的尾数，指数部分的偏置为1023，除此之外双精度浮点和单精度是一致的。 上文发费大量篇幅介绍浮点数的表示方法并非无用功，只有对浮点数的表示足够熟悉才能准确高效地实现浮点量化。当前大多深度学习算法都是基于单精度浮点数进行训练和推理的，而浮点量化就是用低比特的浮点数替代单精度浮点。常见的低比特浮点数有FP16和FP8，前者是16比特的浮点，后者是8比特的浮点。每种浮点数又可以定义不同的格式，比如FP16可以选择5比特指数部分和10比特尾数部分，也可以选择6比特指数部分和9比特尾数部分。不同的格式选择实际上体现了范围和精度之间的权衡。目前基于FP16的训练已经比较成熟，该方案最早由英伟达和百度在“混合精度训练”一文中提出。该文章在权重、激活值和梯度均采用了FP16格式（具体来说是IEEE 754-2008中定义的半精度浮点格式），所实现的精度基本达到或超过单精度浮点模型的基准。 卷积神经网络量化本节主要对卷积神经网络中权重、激活值和梯度三个部分的量化展开介绍。待续。 权重的量化激活值的量化梯度的量化 参考文献Ristretto: Hardward-Oriented Approximation of Convolution Neural Networks Lower Numerical Precision Deep Learning Inference and Training Quantizing deep convolutional networks for efficient inference: A whitepaper","link":"/2019/11/30/quantization/"}],"tags":[{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"quantization","slug":"quantization","link":"/tags/quantization/"}],"categories":[{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"}]}